성능
1) 모델: FP16 / KV: FP16
-> Happy.
2) 모델: FP16 / KV: FP8
-> 단점: Input - Output

A) 16비트 어텐션 연산 끝 = 16비트 배열
B) 16비트 배열 -> 8비트로 내려요. (연산량 적음)
C) KV 캐시 저장
D) Cache hit 발생!!!!!
E) 8비트 어텐션 배열 -> 16비트로 연산(오차 있음)
> 오차 = 확률 -> 토큰이 다른게 나올 수 있어요.
 > 그럼 어떻게 줄여야 할까요?
 > 선택 가능한 토큰을 줄여줍니다. top_k
 > 선택 가능한 확률을 늘려줍니다. top_p
F) Input에 추가 = [KV][Token]

3) 모델: FP8 / KV: FP16 -> 이런 케이스는 없어야합니다.
4) 모델: FP8 / KV: FP8
-> Happy.

---

모델 성능이 꽤 많이 내려와있습니다.
KV 캐시?? 4비트 된건 vLLM 지원 없고
양자화 프레임워크에서도 지원하는지는 각각 다릅니다.

5) 모델: FP4 / KV: FP16
6) 모델: FP4 / KV: FP8
7) 모델: FP4 / KV: FP4
-> 속도 빠르다. 작업이 어렵지 않다.
  (Qwen2.5-14B) -> 4비트 양자화 -> 9 * 8 * 2  = 144개 "자연어로 입력되는 분류"에 썻습니다.
  >> Qwen2.5-3B 파인튜닝: 돈 엄청쓰고 중국어 남발
  >> 7B 8비트: 애매했음. 85%~90%
  >> 14B 4비트: 90%, "중국어"


